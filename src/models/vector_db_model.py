#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VectorDB Model - ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨ ì‘ì—…ì„ ë‹´ë‹¹í•˜ëŠ” ëª¨ë¸
"""

import os
import shutil
from datetime import datetime
from typing import Optional, List, Dict, Any
import tiktoken

try:
    from langchain.document_loaders import PyPDFLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_openai import OpenAIEmbeddings, ChatOpenAI
    from langchain.vectorstores import Chroma
    from langchain.prompts import ChatPromptTemplate
    from langchain.chains import RetrievalQA
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("âš ï¸  LangChain not installed. VectorDB features will be limited.")


class VectorDBModel:
    """VectorDB ê´€ë ¨ ì‘ì—…ì„ ì²˜ë¦¬í•˜ëŠ” ëª¨ë¸ í´ë˜ìŠ¤"""
    
    def __init__(self, chroma_dir: str, pdf_guide_dir: str, openai_api_key: str):
        """
        VectorDBModel ì´ˆê¸°í™”
        
        Args:
            chroma_dir (str): ChromaDB ì €ì¥ ë””ë ‰í† ë¦¬
            pdf_guide_dir (str): PDF ê°€ì´ë“œ íŒŒì¼ ë””ë ‰í† ë¦¬
            openai_api_key (str): OpenAI API í‚¤
        """
        self.chroma_dir = chroma_dir
        self.pdf_guide_dir = pdf_guide_dir
        self.openai_api_key = openai_api_key
        self.vectordb = None
        self.qa_chain = None

    def chroma_db_exists(self) -> bool:
        """ChromaDBê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        return os.path.isfile(os.path.join(self.chroma_dir, "chroma.sqlite3"))

    def initialize_vectordb(self):
        """VectorDBë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        if not LANGCHAIN_AVAILABLE or not self.openai_api_key:
            print("Warning: VectorDB initialization skipped - missing dependencies or API key")
            return None
            
        if self.chroma_db_exists():
            print("âš¡ ê¸°ì¡´ VectorDBê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. batch/bulk ì„ë² ë”© ê±´ë„ˆëœ€.")
            return self.load_existing_vectordb()
        
        # PDF íŒŒì¼ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸
        if not os.path.exists(self.pdf_guide_dir):
            print(f"Warning: PDF guide directory not found: {self.pdf_guide_dir}")
            return None
        
        pdf_files = [f for f in os.listdir(self.pdf_guide_dir) if f.lower().endswith('.pdf')]
        if not pdf_files:
            print(f"Warning: No PDF files found in {self.pdf_guide_dir}")
            return None
        
        print(f"Found {len(pdf_files)} PDF files. Initializing VectorDB...")
        
        embeddings = OpenAIEmbeddings()
        
        # PDF íŒŒì¼ ë¡œë“œ
        docs = []
        for pdf_file in pdf_files:
            file_path = os.path.join(self.pdf_guide_dir, pdf_file)
            try:
                loader = PyPDFLoader(file_path)
                docs.extend(loader.load())
                print(f"Loaded: {pdf_file}")
            except Exception as e:
                print(f"Error loading {pdf_file}: {e}")

        if not docs:
            print("No documents were loaded successfully.")
            return None

        # ë¬¸ì„œ ë¶„í• 
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
        all_chunks = splitter.split_documents(docs)

        # ë°°ì¹˜ë³„ ì„ë² ë”© ë° ì €ì¥
        for i, batch_chunks in enumerate(self._batch_by_token_limit(all_chunks, 290000)):
            db = Chroma.from_documents(
                documents=batch_chunks,
                embedding=embeddings,
                persist_directory=self.chroma_dir
            )
            db.persist()
            del db
            print(f"Batch {i+1} ì €ì¥ ì™„ë£Œ (chunks: {len(batch_chunks)})")
        
        print("âœ… ì‹ ê·œ ì„ë² ë”© ë° VectorDB ìƒì„± ì™„ë£Œ")
        return self.load_existing_vectordb()

    def load_existing_vectordb(self):
        """ê¸°ì¡´ VectorDBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        if not LANGCHAIN_AVAILABLE or not self.openai_api_key:
            return None
            
        try:
            embeddings = OpenAIEmbeddings()
            self.vectordb = Chroma(
                persist_directory=self.chroma_dir,
                embedding_function=embeddings
            )
            print("VectorDB ë¡œë”© ì™„ë£Œ!")
            return self.vectordb
        except Exception as e:
            print(f"Error loading VectorDB: {e}")
            return None

    def initialize_qa_chain(self):
        """QA Chainì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        if not LANGCHAIN_AVAILABLE or not self.vectordb or not self.openai_api_key:
            return None
            
        try:
            llm = ChatOpenAI(
                model_name="gpt-4o-mini",
                temperature=0.3
            )

            prompt = ChatPromptTemplate.from_template(
            """
            ì•„ë˜ ë¬¸ì„œ ì°¸ê³ (context)í•´ì„œ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ, êµ¬ì²´ì ìœ¼ë¡œ ë‹µí•´ì¤˜.
            <context>
            {context}
            </context>
            ì§ˆë¬¸: {question}
            """
            )

            retriever = self.vectordb.as_retriever(search_kwargs={"k": 4})

            self.qa_chain = RetrievalQA.from_chain_type(
                llm=llm,
                retriever=retriever,
                return_source_documents=True,
                chain_type_kwargs={
                    "prompt": prompt
                }
            )
            
            print("QA Chain ì´ˆê¸°í™” ì™„ë£Œ!")
            return self.qa_chain
        except Exception as e:
            print(f"Error initializing QA chain: {e}")
            return None

    def query(self, question: str) -> Dict[str, Any]:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            question (str): ì§ˆë¬¸
            
        Returns:
            Dict[str, Any]: ë‹µë³€ ë° ì†ŒìŠ¤ ë¬¸ì„œ ì •ë³´
        """
        if not self.qa_chain:
            return {
                "result": "í˜„ì¬ ë¬¸ì„œ ê²€ìƒ‰ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
                "source_documents": []
            }
        
        try:
            return self.qa_chain.invoke({"query": question})
        except Exception as e:
            print(f"Error in QA query: {e}")
            return {
                "result": "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì§ˆë¬¸ ì²˜ë¦¬ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.",
                "source_documents": []
            }

    def _batch_by_token_limit(self, chunks, max_tokens=290000, model_name="text-embedding-3-small"):
        """í† í° ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ì²­í¬ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤."""
        if not LANGCHAIN_AVAILABLE:
            yield chunks
            return
            
        tokenizer = tiktoken.encoding_for_model(model_name)
        curr_batch = []
        curr_tokens = 0
        for chunk in chunks:
            text = chunk.page_content if hasattr(chunk, "page_content") else str(chunk)
            tokens = len(tokenizer.encode(text))
            if curr_tokens + tokens > max_tokens and curr_batch:
                yield curr_batch
                curr_batch = []
                curr_tokens = 0
            curr_batch.append(chunk)
            curr_tokens += tokens
        if curr_batch:
            yield curr_batch

    def backup_db(self, backup_dir: str, description: str = "") -> tuple:
        """
        ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë°±ì—…í•©ë‹ˆë‹¤.
        
        Args:
            backup_dir (str): ë°±ì—… ë””ë ‰í† ë¦¬
            description (str): ë°±ì—… ì„¤ëª…
            
        Returns:
            tuple: (ì„±ê³µ ì—¬ë¶€, ë©”ì‹œì§€)
        """
        try:
            now = datetime.now().strftime("%Y%m%d-%H%M%S")
            ver_folder = f"{now}-{description.strip()}" if description.strip() else now
            target_path = os.path.join(backup_dir, ver_folder)
            shutil.copytree(self.chroma_dir, target_path)
            return True, f"ğŸ“¦ ë°±ì—… ì™„ë£Œ: {ver_folder}"
        except Exception as e:
            return False, f"âŒ ë°±ì—… ì‹¤íŒ¨: {str(e)}"

    def restore_db(self, backup_path: str) -> tuple:
        """
        ë°±ì—…ì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë³µì›í•©ë‹ˆë‹¤.
        
        Args:
            backup_path (str): ë°±ì—… íŒŒì¼ ê²½ë¡œ
            
        Returns:
            tuple: (ì„±ê³µ ì—¬ë¶€, ë©”ì‹œì§€)
        """
        try:
            if not os.path.exists(backup_path):
                return False, "âŒ ë°±ì—…ë³¸ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!"
            
            if os.path.exists(self.chroma_dir):
                shutil.rmtree(self.chroma_dir)
            shutil.copytree(backup_path, self.chroma_dir)
            
            # VectorDB ë‹¤ì‹œ ë¡œë“œ
            self.load_existing_vectordb()
            self.initialize_qa_chain()
            
            return True, f"âœ… ë³µêµ¬ ì™„ë£Œ"
        except Exception as e:
            return False, f"âŒ ë³µêµ¬ ì‹¤íŒ¨: {str(e)}"